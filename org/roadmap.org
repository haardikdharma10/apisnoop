# -*- ii: mode; -*-
#+TODO: ORG(o) MOCK(m) TRIAGE(r) BACKLOG(b) TEST(t) SOAK(s) PROMOTION(p) FLAKING(f) COMMENTS(c) | DONE(d)
#+TITLE: APISnoop OKRs

* v1.19
Our goal is to not make radical changes to process or approach, but iterate on our working methods to increase velocity and output in  a stable, consistent way.
** Prepare to Gate cncf/k8s-conformance PRs
- [[https://github.com/cncf/apisnoop/projects/29][cncf/apisnoop/projects/ cncf/k8s-conformance gate]]
*** KR1 Setup prow.cncf.io
This repo is outside kubernetes org.

We'll need to set this up in a sustainable/supportable way.
- [X] DNS prow.cncf.io pointing to prow.apisnoop.io
- [ ] Grant cncf-ci bot permissions to cncf github org
- [ ] Researching the isolation / clusters used by test-infra / k8s-infra-wg
- [ ] Deploy clusters for prow.cncf.io
- [ ] Deploy prow onto said clusters
- [ ] Look into setting up #wg-cncf-infra if there is interest
*** KR2 Connect cncf/k8s-conformance to prow.cncf.io
Comments and admin actions from prow.cncf.io

Will be made using the [[https://github.com/cncf-ci][cncf-ci]] bot/github account.
- [ ] Enable meow and simple prow bot plugins
*** KR3 gate+comment w/ list of unrun conformance tests
- [ ] generate list of test run in a PR
- [ ] generate list of tests required for PR version
- [ ] comment with list of missing tests if under 20
- [ ] comment with count of missing test if over 20
- [ ] add hold tag if test lists don't match
**** definition
Defined by the [[https://github.com/kubernetes/enhancements/blob/2c19ec7627e326d1c75306dcaa3d2f14002301fa/keps/sig-architecture/960-conformance-behaviors/README.md#role-cncf-conformance-program][user stories for KEP-960]]

#+begin_example
Must confirm that the version of the tests being run matches...
Must confirm the set of tests being run matches...
Must confirm that all behaviors are covered by a test...
#+end_example

** Prepare to Gate k/k PRs touching test/e2e or API
- [[https://github.com/cncf/apisnoop/projects/30][k/k API+Conformance Gate]]

Influenced by [[https://github.com/kubernetes/enhancements/pull/1666/files?short_path=92a9412#diff-92a9412ae55358378bc66295cdbea103][Behavior KEP user stories]]

while continuing to focus on endpoints.

Will show increase in endpoints, and tested endpoints, but also, explicitly, whether conformance coverage increased.  Be able to say "you are adding a new endpoint to stable, and you are adding a conformance test for it"

We will investigate the method for ok'ing a pr, and who has the go-ahead to do this.  Bot would likely manage the approval, but done through labels applied by people in owners file or some other set convention.


*** KR1 Identify a PR as requiring conformance review
PR must touch file in conformance-specific directory

- (initially /area-conformance + /sig-arch)
- [ ] Create ~run_if_changed~ presubmit

#+begin_example
eg: update test/conformance/behaviors/..
eg: mv from test/e2e to test/conformance
#+end_example
*** KR2 Identify list of endpoints added/removed
Tooling will compare ~path/operation_id~ in ~api/openapi-spec/swagger.json~
- [ ] Generate list of new endpoints
- [ ] Generate list of removed endpoints
*** KR3 Run APISnoop against PR to generate endpoint coverage
Tooling will provide a list of tested and conformant endpoints.
- [ ] Wait for main prow job to finish
- [ ] Generate list of hit/tested endpoints
- [ ] Generate list of conformant endpoints
*** KR4 bot comment w/ list of increase/decrease of endpoints
Tooling will comment directly on PR

- [ ] alpha : comment endpoints needing tests
- [ ] beta : comment endpoints needing tests
- [ ] stable : comment and block via tag
*** KR5 Manual Approval for SIG-Arch (or appropriate owners)
Ensure the API Review process has been followed.

- [ ] Get feedback on approval process from SIG-Arch
- [ ] Ensure the correct tagging / OWNERS are respected
*** KR6 Donate APISnoop to sig-arch
- [ ] Get feedback if this is desired
- [ ] Get as to location of repo under k8s org
- [ ] Migration maybe in Q3
** Increase Stable Test Coverage Velocity 50% over 1.18
*** Process and Pipelines Much Cleaner
- Conformance Board [[https://github.com/orgs/kubernetes/projects/9?card_filter_query=author%3Ariaankl][ALL]] / [[https://github.com/orgs/kubernetes/projects/9?card_filter_query=author%3Ariaankl][ii]]
- Riaan now manages the board
*** Tooling Requests blocked our existing Promotions
- [[https://github.com/kubernetes/kubernetes/issues/90957][Watch Tooling]] -> 16
- [[https://github.com/kubernetes/kubernetes/issues/90957][ReplicationController Tooling]] -> 7
*** 16 Endpoints decidedly NOT part of Conformance test
- These Endpoints will not be part of Conformance
- APISnoop has been updated this +3.6% increase in â€˜coverage
*** New [[https://github.com/kubernetes/sig-release/tree/master/releases/release-1.19#timeline][Timelines]] due to Covid:
Not effecting the velocity of ii

The group decided to set targets over relases VS quarters.
These numbers are now targeted for over 1.19 instead of Q2.

- Test need to be in Week 14 (July 16th)
- Test Freeze Week 16 (July 30th)
*** Understading when we gain or lose points
- which endpoints were taken away
- which endpoints were added
*** KR1 (20/+40) new conformant stable endpoints
**** 20 DONE
- +4 Merged
- +16 Denominator Points Removed
**** X IN-PROGRESS
- +3 SOAK
*** KR2 +9% Coverage Increase
*** KR3 (stretch) 50% stable endpoints hit by conformance tests

* 2020 Q1 (Jan-Mar)
** Increase Stable Test Coverage Velocity 100% over 2019 (Score:0.4)

We have the remaining 16 endpoints already soaking in the two weeks no flakes window.

We are confident they will merge shortly.
*** KR1=0.4 (11/+27) new conformant stable endpoints
**** SCORE CALCULATION: 0.4 -> 1.0
Done(11)
+ Needs Two Weeks(16) = 27 -> 1.0
**** done = 11                                                     :noexport:
***** done +3 promote: secret patching test #87262
      closed: [2020-04-02 thu 06:49]
***** done +1 promote: find kubernetes service in default namespace #87260
      closed: [2020-04-02 thu 06:50]
***** done +1 promote: namespace patch test #87256
      closed: [2020-04-02 thu 06:50]
***** done +3 promote: pod preemptionexecutionpath verification #83378
- promotion:  https://github.com/kubernetes/kubernetes/pull/83378
***** done +3 promote: podtemplate lifecycle test #88036
- issue: https://github.com/kubernetes/kubernetes/issues/86141
- test: https://github.com/kubernetes/kubernetes/pull/87219
- promotion: https://github.com/kubernetes/kubernetes/pull/88036
**** needs two weeks (no flakes) +16                               :noexport:
***** soak +5 promote: event lifecycle test
- mock-test: jan 6th  https://github.com/kubernetes/kubernetes/issues/86288
- test: april 1st  https://github.com/kubernetes/kubernetes/pull/86858
- promotion:  https://github.com/kubernetes/kubernetes/pull/89753
give the reviewer all the information all we need
- [[https://testgrid.k8s.io/sig-release-master-blocking#gce-cos-master-default&include-filter-by-regex=should%2520ensure%2520that%2520an%2520event%2520can%2520be%2520fetched%252c%2520patched%252c%2520deleted%252c%2520and%2520listed][testgrid reference]]
***** soak +7 promote: replicationcontroller lifecycle
- mock-test:  https://github.com/kubernetes/kubernetes/issues/88302
  needs reopening and checkboxes for current state...
- test:  https://github.com/kubernetes/kubernetes/pull/88588
- promotion:

- [[https://github.com/kubernetes/kubernetes/issues/89740][address flaking comment]] : [[https://github.com/kubernetes/kubernetes/pull/89746][https://github.com/kubernetes/kubernetes/pull/89746]]
relies on it's own update response data
> i have the same concern as #89707 that this test will not fail if the watch times out
***** soak +4 promote: endpoints
- mock-test: feb 3rd  https://github.com/kubernetes/kubernetes/issues/87762
- test: mar 27th https://github.com/kubernetes/kubernetes/pull/88778
- promotion: april 10th? https://github.com/kubernetes/kubernetes/pull/89752
- [[https://testgrid.k8s.io/sig-release-master-blocking#gce-cos-master-default&include-filter-by-regex=should%2520test%2520the%2520lifecycle%2520of%2520an%2520endpoint][testgrid reference]] still looks green!
fixme: create shows +5^, mock+promotion shows +4
same issue as configmap lificle:
this doesn't verify that the endpoints is deleted.
it just watches for an endpoints deletion event.
would this test fail if it didn't see a deletion event?
**** needs review +6                                               :noexport:
***** comments +2 promote: configmap lifecycle test #88034 (comments addressed)
conceptually this pr adds watches
there's no gaurantee that we will see the watch.
let's ensure what happens in the negative case.
when your waiting for the config map to be deleted, how do you know it's not deleted.
for each watch:
what happens if the watch times out...
when you setup a watch to timeout after 60 seconds....
pretend it's running on a super slow processor
what if it times out for every single test.... would i want the watch to be considered a failure....
probably... if it doesn't execute to completion.
it's not clear that that happens
- promotion: https://github.com/kubernetes/kubernetes/pull/88034#discussion_r398728147
- addressing comments: https://github.com/kubernetes/kubernetes/pull/88034#issuecomment-607430447 (addresed)
- pr to handle timeouts: https://github.com/kubernetes/kubernetes/pull/89707
***** comments +4 pod and podstatus
- mock-test:  https://github.com/kubernetes/kubernetes/issues/88545
- test:  https://github.com/kubernetes/kubernetes/pull/89453
  addressed the [[https://github.com/kubernetes/kubernetes/pull/89453#discussion_r400346746][comment]]:
  "not sure this will work, you will be racing with the kubelet, i think. that is, kubelet may mark it ready again."
**** sorted backlog +5                                             :noexport:
***** backlog +2 servicestatus lifecycle
- org-file: https://github.com/cncf/apisnoop/pull/298
- mock-test: https://github.com/kubernetes/kubernetes/issues/89135
 currently, this test is having issues writing to the servicestatus endpoints (via patch and update).
 the data is patched without errors, but the data when fetched is no different to before the patching.
***** backlog +3 serviceaccount lifecycle
- mock-test: https://github.com/kubernetes/kubernetes/issues/89071
 @johnbelamaric you don't need to check the status of the secret as part of the test. in other places we check that the resource in question happens, we don't have to follow.
**** triage +12                                                    :noexport:
***** triage +5 apps daemonset lifecycle
- org-file: https://github.com/cncf/apisnoop/pull/305
- mock-test: https://github.com/kubernetes/kubernetes/issues/89637
***** triage +5 apps deployment lifecycle
- org-file:
- mock-test: https://github.com/kubernetes/kubernetes/issues/89340
***** triage +2 nodestatus                                    :deprioritized:
      needs these comments addressed, and we voted to de-priorize
  https://github.com/kubernetes/kubernetes/issues/88358#issuecomment-591062171

*** kr2=0.4 +6% coverage increase
**** SCORE CALCULATION: 0.4 -> 1.0
This number should increase to the full 6% in ~2 weeks.
** complete cncf/apisnoop prow.k8s.io + Amazon migration (Score:0.5)
*** KR1=0.5 All cncf/apisnoop artifacts created by prow.k8s.io
Definitions in prow, but need to do our Q1 release... this week.
**** search for apisnoop in kubernetes/test-infra
https://github.com/kubernetes/test-infra/search?q=apisnoop&unscoped_q=apisnoop
**** 4 postsubmits that [[https://github.com/kubernetes/test-infra/blob/master/prow/jobs.md#how-to-configure-new-jobs][run after merging code]]

We currently have four postsubmit jobs defined in [[https://github.com/kubernetes/test-infra/blob/c8eafffeadbd18617b071adb4dd3d7b900f06fa5/config/jobs/image-pushing/k8s-staging-apisnoop.yaml#L2][config/jobs/image-pushing/k8s-staging-apisnoop.yaml]]

They are all variations of:

#+begin_src yaml
postsubmits:
  cncf/apisnoop:
    - name: apisnoop-push-webapp-images
      cluster: test-infra-trusted
      annotations:
        testgrid-dashboards: conformance-apisnoop
        testgrid-tab-name: apisnoop-webapp-image
        testgrid-alert-email: apisnoop@ii.coop
        description: Builds the webapp image for APISnoop deployments
      decorate: true
      branches:
        - ^master$
      spec:
        serviceAccountName: deployer # TODO(fejta): use pusher
        containers:
          - image: gcr.io/k8s-testimages/image-builder:v20200213-0032cdb
            command:
              - /run.sh
            args:
              # this is the project GCB will run in, which is the same as the GCR images are pushed to.
              - --project=k8s-staging-apisnoop
              - --scratch-bucket=gs://k8s-staging-apisnoop-gcb
              - --env-passthrough=PULL_BASE_REF
              - apps/webapp/app
#+end_src
**** testgrid dashboard group
- [[https://github.com/kubernetes/test-infra/blob/98958caf0044dbe3c751c909eac861f0cbf5738f/config/testgrids/conformance/conformance-all.yaml#L5][test-infra/config/testgrids/conformance/conformance-all.yaml]]
#+begin_src yaml
dashboard_groups:
- name: conformance
  dashboard_names:
    - conformance-all
    - conformance-apisnoop
#+end_src
**** testgrid dashboards
- [[https://github.com/kubernetes/test-infra/blob/98958caf0044dbe3c751c909eac861f0cbf5738f/config/testgrids/conformance/conformance-all.yaml#182][test-infra/config/testgrids/conformance/conformance-all.yaml]]
#+begin_src yaml
dashboards:
- name: conformance-all
  # entries are named $PROVIDER, $KUBERNETES_RELEASE
  dashboard_tab:
  - name: conformance-apisnoop
#+end_src
*** KR2=0.0 All cncf/apisnoop github workflow managed by prow.k8s.io
- [ ] PR Merged managed via prow (VS pushing to master or manual merging)
**** configure [[https://github.com/kubernetes/test-infra/blob/2ac98631f533986f1d4b6cf8cb02d2f38f34f2b6/config/prow/plugins.yaml#L890-L905][test-infra/prow/config/plugins.yaml]]
- [ ] Remove ability to push to branches
- [ ] enforce usage of PRs
- [ ] remove ability to merge
- [ ] add/enable owners files
- [ ] if tests don't pass, pr is blocked
- [ ] enforce lgtm + approve blocks
- [ ] k8s-bot merges the PRs
*** KR3=1.0 All cncf/apisnoop non-prow infra moved to Amazon/Packet
We aren't hosting anything on Google (except via prow).

Everything is on EKS on Packet!
** Mentor/Teach test-writing workflow at Contributer Summit / KubeConEU (Score:0.5)
*** KR1=0.0 Caleb and Hippie Mentoring at Contributor Summit
I am pairing weekly with with k8s community members.

To ensure the workflow is accessible.

Caleb is mentoring Zach and Stephen.
*** KR2 1.0 Zach and Stephen teaching test writing
They in turn are teaching Riaan

all remote

using our org-flow
* Footnotes

#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
# #+REVEAL_TITLE_SLIDE:
#+NOREVEAL_DEFAULT_FRAG_STYLE: YY
#+NOREVEAL_EXTRA_CSS: YY
#+NOREVEAL_EXTRA_JS: YY
#+REVEAL_HLEVEL: 2
#+REVEAL_MARGIN: 0.1
#+REVEAL_WIDTH: 1000
#+REVEAL_HEIGHT: 600
#+REVEAL_MAX_SCALE: 3.5
#+REVEAL_MIN_SCALE: 0.2
#+REVEAL_PLUGINS: (markdown notes highlight multiplex)
#+REVEAL_SLIDE_NUMBER: ""
#+REVEAL_SPEED: 1
#+REVEAL_THEME: sky
#+REVEAL_THEME_OPTIONS: beige|black|blood|league|moon|night|serif|simple|sky|solarized|white
#+REVEAL_TRANS: cube
#+REVEAL_TRANS_OPTIONS: none|cube|fade|concave|convex|page|slide|zoom

#+OPTIONS: num:nil
#+OPTIONS: toc:nil
#+OPTIONS: mathjax:Y
#+OPTIONS: reveal_single_file:nil
#+OPTIONS: reveal_control:t
#+OPTIONS: reveal-progress:t
#+OPTIONS: reveal_history:nil
#+OPTIONS: reveal_center:t
#+OPTIONS: reveal_rolling_links:nil
#+OPTIONS: reveal_keyboard:t
#+OPTIONS: reveal_overview:t
